# -*- coding: utf-8 -*-
"""student_dropout.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MKangabire/student_dropout/blob/main/student_dropout.ipynb
"""
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras.regularizers import l2, l1
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/student dropout.csv')

df

columns_to_keep = [
    'Dropped_Out', 'Mother_Education', 'Father_Education', 'Final_Grade', 'Grade_1',
    'Grade_2', 'Number_of_Failures', 'School', 'Wants_Higher_Education', 'Study_Time',
    'Weekend_Alcohol_Consumption', 'Weekday_Alcohol_Consumption', 'Address',
    'Reason_for_Choosing_School'

]
df = df[columns_to_keep]
df.head()

categories = df.select_dtypes(include=['object']).columns

df_new = pd.get_dummies(df, columns=categories, drop_first=True)
df_new.head()

import pickle
category_mapping = {}
for column in categories:
    unique_values = df[column].unique()
    for value in unique_values:
        one_hot_column_name = f"{column}_{value}"
        category_mapping[column + '_' + value] = one_hot_column_name

with open('category_mapping.pkl', 'wb') as file:
    pickle.dump(category_mapping, file)

with open('category_mapping.pkl', 'rb') as file:
    category_mapping = pickle.load(file)

def map_input_row(input_row, category_mapping):
    """
    Maps a row of input features (including categorical features) to their
    corresponding one-hot encoded values.

    Args:
        input_row: A dictionary representing a row of input features.
        category_mapping: The dictionary containing the mapping.

    Returns:
        A pandas Series containing the one-hot encoded values,
        or None if no mapping is found for any feature.
    """
    encoded_values = pd.Series(0, index=category_mapping.values())

    for feature_name, feature_value in input_row.items():
        if feature_name in set([k.split('_')[0] for k in category_mapping]):  # If feature is categorical
            one_hot_column_name = category_mapping.get(feature_name + '_' + feature_value)

            if one_hot_column_name:
                encoded_values[one_hot_column_name] = 1
            else:
                print(f"Warning: No mapping found for {feature_name}: {feature_value}")
                # You might want to handle missing mappings differently here

    return encoded_values

correlation = df_new.corr()[['Dropped_Out']]
plt.figure(figsize=(6,12))
sns.heatmap(correlation, cmap='coolwarm', annot=True, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

x = df_new.drop(columns=['Dropped_Out'])
y = df_new['Dropped_Out']

scaler = StandardScaler()
X_scaled =scaler.fit_transform(x)
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
print("Training set:", X_train.shape, y_train.shape)
print("Validation set:", X_val.shape, y_val.shape)
print("Testing set:", X_test.shape, y_test.shape)
print(f"Training samples: {X_train.shape[0]}")
print(f"Validation samples: {X_val.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

def model():
    # Define the model
    model = Sequential()
    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.006)))
    model.add(BatchNormalization())
    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.006)))
    model.add(BatchNormalization())
    model.add(Dropout(0.4))
    # model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def train_model(model, X_train, y_train, X_val, y_val):
    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=400, verbose=2, callbacks=[early_stopping])
    return history
model = model()
history = train_model(model, X_train, y_train, X_val, y_val)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
y_pred = (model.predict(X_test) > 0.5).astype("int32")
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# plot training history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using Seaborn heatmap
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Not Dropped Out', 'Dropped Out'], yticklabels=['Not Dropped Out', 'Dropped Out'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

import pickle
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

model.save('model.keras')